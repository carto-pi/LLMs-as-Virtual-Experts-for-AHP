{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf384a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, gmean\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "class OrderingImpactAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze the impact of criteria presentation order on LLM performance \n",
    "    in multi-criteria decision making tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Expert benchmark weights (can be modified for different applications)\n",
    "        self.expert_weights = {\n",
    "            'Solar Radiation': 0.235,\n",
    "            'Aspect': 0.146,\n",
    "            'Aspect (Slope Direction)': 0.146,  # Keep both for compatibility\n",
    "            'Land Use': 0.144,\n",
    "            'Slope': 0.075,\n",
    "            'Distance to Power Transmission Lines': 0.064,\n",
    "            'Distance to Settlements': 0.064,\n",
    "            'Distance to Transformers': 0.058,\n",
    "            'Altitude': 0.048,\n",
    "            'Distance to Protected Areas': 0.040,\n",
    "            'Distance to Highways': 0.035,\n",
    "            'Distance from Fault Lines': 0.029,\n",
    "            'Distance to Airports': 0.022,\n",
    "            'Distance to Rivers and Lakes': 0.022,\n",
    "            'Distance to Bird Migration Routes': 0.018\n",
    "        }\n",
    "    \n",
    "    def safe_geometric_mean(self, values):\n",
    "        \"\"\"\n",
    "        Calculate geometric mean safely, handling zeros and negative values.\n",
    "        \"\"\"\n",
    "        values = np.array(values)\n",
    "        \n",
    "        if np.any(values <= 0):\n",
    "            warnings.warn(\"Found zero or negative values. Using arithmetic mean instead.\")\n",
    "            return np.mean(values)\n",
    "        \n",
    "        try:\n",
    "            return gmean(values)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error calculating geometric mean: {e}. Using arithmetic mean.\")\n",
    "            return np.mean(values)\n",
    "    \n",
    "    def calculate_correlation_with_benchmark(self, model_weights, criteria_names):\n",
    "        \"\"\"Calculate Pearson correlation with benchmark weights\"\"\"\n",
    "        \n",
    "        benchmark_weights_ordered = []\n",
    "        model_weights_ordered = []\n",
    "        \n",
    "        for criterion in criteria_names:\n",
    "            if criterion in self.expert_weights:\n",
    "                benchmark_weights_ordered.append(self.expert_weights[criterion])\n",
    "                model_weights_ordered.append(model_weights[criterion])\n",
    "            else:\n",
    "                print(f\"Warning: Criterion '{criterion}' not found in benchmark weights\")\n",
    "        \n",
    "        if len(benchmark_weights_ordered) != len(model_weights_ordered):\n",
    "            print(f\"Warning: Mismatch in number of criteria\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            pearson_r, _ = pearsonr(benchmark_weights_ordered, model_weights_ordered)\n",
    "            return pearson_r\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating correlation: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_random_order_performance(self, consolidated_df, baseline_approach=\"minimal\"):\n",
    "        \"\"\"\n",
    "        Get random order performance using geometric mean aggregation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find baseline approach\n",
    "        baseline_data = None\n",
    "        for approach in consolidated_df['Prompting_Approach'].unique():\n",
    "            approach_lower = approach.lower().replace('_', ' ')\n",
    "            if baseline_approach.lower() in approach_lower:\n",
    "                baseline_data = consolidated_df[consolidated_df['Prompting_Approach'] == approach]\n",
    "                print(f\"Found random order approach: {approach}\")\n",
    "                break\n",
    "        \n",
    "        if baseline_data is None:\n",
    "            print(\"Warning: Using first available approach as random order\")\n",
    "            first_approach = consolidated_df['Prompting_Approach'].unique()[0]\n",
    "            baseline_data = consolidated_df[consolidated_df['Prompting_Approach'] == first_approach]\n",
    "            print(f\"Using approach: {first_approach}\")\n",
    "        \n",
    "        # Get criteria columns\n",
    "        criteria_columns = [col for col in baseline_data.columns \n",
    "                          if col not in ['Prompting_Approach', 'Model', 'Iteration']]\n",
    "        \n",
    "        # Calculate geometric mean weights for each model\n",
    "        models = baseline_data['Model'].unique()\n",
    "        random_order_results = {}\n",
    "        \n",
    "        for model in models:\n",
    "            model_data = baseline_data[baseline_data['Model'] == model]\n",
    "            \n",
    "            geometric_mean_weights = {}\n",
    "            for col in criteria_columns:\n",
    "                values = model_data[col].values\n",
    "                geometric_mean_weights[col] = self.safe_geometric_mean(values)\n",
    "            \n",
    "            correlation = self.calculate_correlation_with_benchmark(\n",
    "                geometric_mean_weights, criteria_columns\n",
    "            )\n",
    "            if correlation is not None:\n",
    "                random_order_results[model] = correlation\n",
    "        \n",
    "        return random_order_results\n",
    "    \n",
    "    def get_expert_aligned_performance(self, expert_aligned_df):\n",
    "        \"\"\"Get performance for expert-aligned ordering condition\"\"\"\n",
    "        \n",
    "        expert_aligned_results = {}\n",
    "        model_columns = [col for col in expert_aligned_df.columns if col != 'Criterion']\n",
    "        \n",
    "        for model_col in model_columns:\n",
    "            # Clean model name\n",
    "            model_name = model_col.replace('-', '').replace('_', '')\n",
    "            \n",
    "            # Create weight dictionary\n",
    "            model_weights = {}\n",
    "            for idx, row in expert_aligned_df.iterrows():\n",
    "                criterion = row['Criterion']\n",
    "                weight = row[model_col]\n",
    "                model_weights[criterion] = weight\n",
    "            \n",
    "            # Calculate correlation\n",
    "            criteria_names = expert_aligned_df['Criterion'].tolist()\n",
    "            correlation = self.calculate_correlation_with_benchmark(\n",
    "                model_weights, criteria_names\n",
    "            )\n",
    "            \n",
    "            if correlation is not None:\n",
    "                expert_aligned_results[model_name] = correlation\n",
    "        \n",
    "        return expert_aligned_results\n",
    "    \n",
    "    def normalize_model_name(self, name):\n",
    "        \"\"\"Normalize model names for comparison\"\"\"\n",
    "        return name.lower().replace('-', '').replace('_', '').replace('.', '').replace(' ', '')\n",
    "    \n",
    "    def match_models(self, random_order_results, expert_aligned_results):\n",
    "        \"\"\"Match models between random order and expert-aligned conditions\"\"\"\n",
    "        \n",
    "        random_order_models = list(random_order_results.keys())\n",
    "        expert_aligned_models = list(expert_aligned_results.keys())\n",
    "        \n",
    "        matched_models = []\n",
    "        matched_random = set()\n",
    "        matched_expert = set()\n",
    "        \n",
    "        # Exact matches first\n",
    "        for expert_model in expert_aligned_models:\n",
    "            expert_normalized = self.normalize_model_name(expert_model)\n",
    "            \n",
    "            for random_model in random_order_models:\n",
    "                random_normalized = self.normalize_model_name(random_model)\n",
    "                \n",
    "                if expert_normalized == random_normalized:\n",
    "                    matched_models.append((expert_model, random_model))\n",
    "                    matched_random.add(random_model)\n",
    "                    matched_expert.add(expert_model)\n",
    "                    break\n",
    "        \n",
    "        # Flexible matching for unmatched models\n",
    "        unmatched_expert = [m for m in expert_aligned_models if m not in matched_expert]\n",
    "        unmatched_random = [m for m in random_order_models if m not in matched_random]\n",
    "        \n",
    "        for expert_model in unmatched_expert:\n",
    "            expert_normalized = self.normalize_model_name(expert_model)\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for random_model in unmatched_random:\n",
    "                random_normalized = self.normalize_model_name(random_model)\n",
    "                \n",
    "                # Calculate similarity\n",
    "                if expert_normalized in random_normalized:\n",
    "                    score = len(expert_normalized) / len(random_normalized)\n",
    "                    if score > best_score and score > 0.8:\n",
    "                        best_score = score\n",
    "                        best_match = random_model\n",
    "                elif random_normalized in expert_normalized:\n",
    "                    score = len(random_normalized) / len(expert_normalized)\n",
    "                    if score > best_score and score > 0.8:\n",
    "                        best_score = score\n",
    "                        best_match = random_model\n",
    "            \n",
    "            if best_match:\n",
    "                matched_models.append((expert_model, best_match))\n",
    "                matched_random.add(best_match)\n",
    "                matched_expert.add(expert_model)\n",
    "                unmatched_random.remove(best_match)\n",
    "        \n",
    "        return matched_models\n",
    "    \n",
    "    def create_display_name(self, model_name):\n",
    "        \"\"\"Convert model name to display format\"\"\"\n",
    "        name_mapping = {\n",
    "            'Claude37': 'Claude-3.7',\n",
    "            'Claude37Thinking': 'Claude-3.7-Thinking',\n",
    "            'DeepSeekR1': 'DeepSeek-R1',\n",
    "            'GPT41': 'GPT-4.1',\n",
    "            'Gemini25Pro': 'Gemini-2.5-Pro',\n",
    "            'o3': 'o3'\n",
    "        }\n",
    "        return name_mapping.get(model_name, model_name)\n",
    "    \n",
    "    def analyze_ordering_impact(self, random_order_weights_path, expert_aligned_weights_path):\n",
    "        \"\"\"\n",
    "        Main analysis function to compare random order vs expert-aligned ordering performance.\n",
    "        \n",
    "        Parameters:\n",
    "        random_order_weights_path (str): Path to random order condition weights\n",
    "        expert_aligned_weights_path (str): Path to expert-aligned condition weights\n",
    "        \n",
    "        Returns:\n",
    "        pd.DataFrame: Results comparing performance between conditions\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            random_order_df = pd.read_csv(random_order_weights_path)\n",
    "            expert_aligned_df = pd.read_csv(expert_aligned_weights_path)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: File not found - {e}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Calculating random order performance...\")\n",
    "        random_order_results = self.get_random_order_performance(random_order_df)\n",
    "        \n",
    "        print(\"Calculating expert-aligned performance...\")\n",
    "        expert_aligned_results = self.get_expert_aligned_performance(expert_aligned_df)\n",
    "        \n",
    "        print(\"Matching models between conditions...\")\n",
    "        matched_models = self.match_models(random_order_results, expert_aligned_results)\n",
    "        \n",
    "        # Create results table\n",
    "        results_data = []\n",
    "        \n",
    "        for expert_model, random_model in matched_models:\n",
    "            random_order_r = random_order_results[random_model]\n",
    "            expert_aligned_r = expert_aligned_results[expert_model]\n",
    "            \n",
    "            delta_r = expert_aligned_r - random_order_r\n",
    "            percent_change = (delta_r / random_order_r) * 100\n",
    "            \n",
    "            display_name = self.create_display_name(expert_model)\n",
    "            \n",
    "            results_data.append({\n",
    "                'Model': display_name,\n",
    "                'Random Order': round(random_order_r, 3),\n",
    "                'Expert-Aligned': round(expert_aligned_r, 3),\n",
    "                'Δ Pearson r': f\"{delta_r:+.3f}\",\n",
    "                '% Change': f\"{percent_change:+.1f}%\"\n",
    "            })\n",
    "        \n",
    "        # Calculate overall average\n",
    "        if results_data:\n",
    "            random_values = [result['Random Order'] for result in results_data]\n",
    "            expert_values = [result['Expert-Aligned'] for result in results_data]\n",
    "            \n",
    "            avg_random = np.mean(random_values)\n",
    "            avg_expert = np.mean(expert_values)\n",
    "            avg_delta = avg_expert - avg_random\n",
    "            avg_percent = (avg_delta / avg_random) * 100\n",
    "            \n",
    "            results_data.append({\n",
    "                'Model': 'Overall Average',\n",
    "                'Random Order': round(avg_random, 3),\n",
    "                'Expert-Aligned': round(avg_expert, 3),\n",
    "                'Δ Pearson r': f\"{avg_delta:+.3f}\",\n",
    "                '% Change': f\"{avg_percent:+.1f}%\"\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        return results_df\n",
    "    \n",
    "    def display_results(self, results_df, title=\"Ordering Impact Analysis\"):\n",
    "        \"\"\"Display results in formatted table\"\"\"\n",
    "        \n",
    "        if results_df is None or results_df.empty:\n",
    "            print(\"No results to display\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(title)\n",
    "        print(\"=\"*80)\n",
    "        print(results_df.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def save_results(self, results_df, output_dir=\"output\", filename=\"ordering_impact_analysis.csv\"):\n",
    "        \"\"\"Save results to CSV file\"\"\"\n",
    "        \n",
    "        if results_df is None or results_df.empty:\n",
    "            print(\"No results to save\")\n",
    "            return\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "    \n",
    "    def get_summary_statistics(self, results_df):\n",
    "        \"\"\"Generate summary statistics from results\"\"\"\n",
    "        \n",
    "        if results_df is None or results_df.empty:\n",
    "            return None\n",
    "        \n",
    "        individual_models = results_df[results_df['Model'] != 'Overall Average']\n",
    "        \n",
    "        if individual_models.empty:\n",
    "            return None\n",
    "        \n",
    "        # Extract numeric values\n",
    "        percent_changes = []\n",
    "        for _, row in individual_models.iterrows():\n",
    "            percent_str = row['% Change']\n",
    "            percent_val = float(percent_str.replace('%', '').replace('+', ''))\n",
    "            percent_changes.append(percent_val)\n",
    "        \n",
    "        summary = {\n",
    "            'models_analyzed': len(individual_models),\n",
    "            'positive_impacts': len([p for p in percent_changes if p > 0]),\n",
    "            'negative_impacts': len([p for p in percent_changes if p < 0]),\n",
    "            'max_improvement': max(percent_changes) if percent_changes else 0,\n",
    "            'max_degradation': min(percent_changes) if percent_changes else 0,\n",
    "            'overall_change': results_df[results_df['Model'] == 'Overall Average']['% Change'].iloc[0] \n",
    "                            if not results_df[results_df['Model'] == 'Overall Average'].empty else \"N/A\"\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = OrderingImpactAnalyzer()\n",
    "    \n",
    "    # File paths - modify these for your data\n",
    "    random_order_weights_path = \"individual_weights_iterations.csv\"\n",
    "    expert_aligned_weights_path = \"expert_aligned_order_weights.csv\"\n",
    "    \n",
    "    print(\"Starting ordering impact analysis...\")\n",
    "    \n",
    "    # Perform analysis\n",
    "    results = analyzer.analyze_ordering_impact(\n",
    "        random_order_weights_path, \n",
    "        expert_aligned_weights_path\n",
    "    )\n",
    "    \n",
    "    if results is not None:\n",
    "        # Display results\n",
    "        analyzer.display_results(results, \"Criteria Ordering Impact on LLM Performance\")\n",
    "        \n",
    "        # Save results\n",
    "        analyzer.save_results(results)\n",
    "        \n",
    "        # Print summary\n",
    "        summary = analyzer.get_summary_statistics(results)\n",
    "        if summary:\n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"- Models analyzed: {summary['models_analyzed']}\")\n",
    "            print(f\"- Models with improvement: {summary['positive_impacts']}\")\n",
    "            print(f\"- Models with degradation: {summary['negative_impacts']}\")\n",
    "            print(f\"- Maximum improvement: +{summary['max_improvement']:.1f}%\")\n",
    "            print(f\"- Maximum degradation: {summary['max_degradation']:+.1f}%\")\n",
    "            print(f\"- Overall average change: {summary['overall_change']}\")\n",
    "        \n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "    else:\n",
    "        print(\"Analysis failed. Please check your input files.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dd668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
