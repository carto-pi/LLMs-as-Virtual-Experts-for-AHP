{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da439e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, gmean\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def calculate_performance_comparison(pairwise_weights_path, direct_weights_path, expert_weights_dict):\n",
    "    \"\"\"\n",
    "    Compare LLM performance between pairwise comparison and direct weight solicitation methods.\n",
    "    \n",
    "    Parameters:\n",
    "    pairwise_weights_path (str): Path to CSV with pairwise comparison weights\n",
    "    direct_weights_path (str): Path to CSV with direct solicitation weights  \n",
    "    expert_weights_dict (dict): Dictionary mapping criterion names to expert weights\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (method_comparison_df, individual_direct_df) - Performance comparison tables\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data files\n",
    "    try:\n",
    "        pairwise_df = pd.read_csv(pairwise_weights_path)\n",
    "        direct_df = pd.read_csv(direct_weights_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Loaded pairwise weights: {pairwise_df.shape}\")\n",
    "    print(f\"Loaded direct weights: {direct_df.shape}\")\n",
    "    \n",
    "    def safe_geometric_mean(values):\n",
    "        \"\"\"Calculate geometric mean safely, handling zeros and negative values.\"\"\"\n",
    "        values = np.array(values)\n",
    "        \n",
    "        if np.any(values <= 0):\n",
    "            warnings.warn(\"Found zero or negative values. Using arithmetic mean instead.\")\n",
    "            return np.mean(values)\n",
    "        \n",
    "        try:\n",
    "            return gmean(values)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error calculating geometric mean: {e}. Using arithmetic mean.\")\n",
    "            return np.mean(values)\n",
    "    \n",
    "    def calculate_metrics(llm_weights, expert_weights_dict, criteria_columns):\n",
    "        \"\"\"Calculate performance metrics between LLM and expert weights.\"\"\"\n",
    "        \n",
    "        expert_values = []\n",
    "        llm_values = []\n",
    "        \n",
    "        for col in criteria_columns:\n",
    "            if col in expert_weights_dict:\n",
    "                expert_values.append(expert_weights_dict[col])\n",
    "                llm_values.append(llm_weights[col])\n",
    "            else:\n",
    "                print(f\"Warning: '{col}' not found in expert weights\")\n",
    "        \n",
    "        if len(expert_values) != len(llm_values):\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        try:\n",
    "            pearson_r, pearson_p = pearsonr(expert_values, llm_values)\n",
    "            spearman_r, spearman_p = spearmanr(expert_values, llm_values)\n",
    "            rmse = np.sqrt(np.mean((np.array(expert_values) - np.array(llm_values))**2))\n",
    "            \n",
    "            return pearson_r, pearson_p, spearman_r, spearman_p, rmse\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics: {e}\")\n",
    "            return None, None, None, None, None\n",
    "    \n",
    "    def process_direct_solicitation():\n",
    "        \"\"\"Process direct solicitation results.\"\"\"\n",
    "        \n",
    "        # Get criteria columns (exclude metadata)\n",
    "        criteria_columns = [col for col in direct_df.columns \n",
    "                          if col not in ['Model', 'Response_Number', 'Iteration']]\n",
    "        \n",
    "        models = direct_df['Model'].unique()\n",
    "        results = []\n",
    "        \n",
    "        print(\"Processing direct solicitation methods...\")\n",
    "        \n",
    "        for model in models:\n",
    "            model_data = direct_df[direct_df['Model'] == model]\n",
    "            \n",
    "            # Calculate geometric mean weights\n",
    "            mean_weights = {}\n",
    "            for col in criteria_columns:\n",
    "                values = model_data[col].values\n",
    "                mean_weights[col] = safe_geometric_mean(values)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            metrics = calculate_metrics(pd.Series(mean_weights), expert_weights_dict, criteria_columns)\n",
    "            \n",
    "            if metrics[0] is not None:\n",
    "                pearson_r, pearson_p, spearman_r, spearman_p, rmse = metrics\n",
    "                \n",
    "                results.append({\n",
    "                    'Model': model,\n",
    "                    'Pearson_r': pearson_r,\n",
    "                    'Pearson_p': pearson_p,\n",
    "                    'Spearman_rho': spearman_r,\n",
    "                    'Spearman_p': spearman_p,\n",
    "                    'RMSE': rmse\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def process_pairwise_approach(approach_name):\n",
    "        \"\"\"Process specific pairwise comparison approach.\"\"\"\n",
    "        \n",
    "        # Filter data\n",
    "        approach_data = pairwise_df[pairwise_df['Prompting_Approach'] == approach_name].copy()\n",
    "        \n",
    "        if approach_data.empty:\n",
    "            print(f\"Warning: No data for approach '{approach_name}'\")\n",
    "            return None\n",
    "        \n",
    "        # Get criteria columns\n",
    "        criteria_columns = [col for col in approach_data.columns \n",
    "                          if col not in ['Prompting_Approach', 'Model', 'Iteration']]\n",
    "        \n",
    "        models = approach_data['Model'].unique()\n",
    "        model_results = []\n",
    "        \n",
    "        for model in models:\n",
    "            model_data = approach_data[approach_data['Model'] == model]\n",
    "            \n",
    "            # Calculate geometric mean weights\n",
    "            mean_weights = {}\n",
    "            for col in criteria_columns:\n",
    "                values = model_data[col].values\n",
    "                mean_weights[col] = safe_geometric_mean(values)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(pd.Series(mean_weights), expert_weights_dict, criteria_columns)\n",
    "            \n",
    "            if metrics[0] is not None:\n",
    "                pearson_r, _, spearman_r, _, rmse = metrics\n",
    "                model_results.append({\n",
    "                    'Pearson_r': pearson_r,\n",
    "                    'Spearman_rho': spearman_r,\n",
    "                    'RMSE': rmse\n",
    "                })\n",
    "        \n",
    "        if model_results:\n",
    "            df = pd.DataFrame(model_results)\n",
    "            return {\n",
    "                'num_models': len(model_results),\n",
    "                'mean_pearson': df['Pearson_r'].mean(),\n",
    "                'std_pearson': df['Pearson_r'].std(),\n",
    "                'mean_spearman': df['Spearman_rho'].mean(),\n",
    "                'std_spearman': df['Spearman_rho'].std(),\n",
    "                'mean_rmse': df['RMSE'].mean(),\n",
    "                'std_rmse': df['RMSE'].std()\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Process direct solicitation\n",
    "    direct_performance = process_direct_solicitation()\n",
    "    \n",
    "    # Create individual results table\n",
    "    individual_results = []\n",
    "    for _, row in direct_performance.iterrows():\n",
    "        individual_results.append({\n",
    "            'Model': row['Model'],\n",
    "            'Pearson_r': round(row['Pearson_r'], 3),\n",
    "            'p_value': \"<0.001\" if row['Pearson_p'] < 0.001 else f\"{row['Pearson_p']:.3f}\",\n",
    "            'Spearman_rho': round(row['Spearman_rho'], 3),\n",
    "            'Spearman_p': \"<0.001\" if row['Spearman_p'] < 0.001 else f\"{row['Spearman_p']:.3f}\",\n",
    "            'RMSE': round(row['RMSE'], 3)\n",
    "        })\n",
    "    \n",
    "    individual_df = pd.DataFrame(individual_results)\n",
    "    \n",
    "    # Calculate summary for direct solicitation\n",
    "    direct_summary = {\n",
    "        'num_models': len(direct_performance),\n",
    "        'mean_pearson': direct_performance['Pearson_r'].mean(),\n",
    "        'std_pearson': direct_performance['Pearson_r'].std(),\n",
    "        'mean_spearman': direct_performance['Spearman_rho'].mean(),\n",
    "        'std_spearman': direct_performance['Spearman_rho'].std(),\n",
    "        'mean_rmse': direct_performance['RMSE'].mean(),\n",
    "        'std_rmse': direct_performance['RMSE'].std()\n",
    "    }\n",
    "    \n",
    "    # Process pairwise approaches\n",
    "    comparison_results = []\n",
    "    \n",
    "    # Add direct solicitation\n",
    "    comparison_results.append({\n",
    "        'Method': 'Direct Solicitation',\n",
    "        'Num_Models': direct_summary['num_models'],\n",
    "        'Mean_Pearson': f\"{direct_summary['mean_pearson']:.3f} ± {direct_summary['std_pearson']:.3f}\",\n",
    "        'Mean_Spearman': f\"{direct_summary['mean_spearman']:.3f} ± {direct_summary['std_spearman']:.3f}\",\n",
    "        'Mean_RMSE': f\"{direct_summary['mean_rmse']:.3f} ± {direct_summary['std_rmse']:.3f}\"\n",
    "    })\n",
    "    \n",
    "    # Add pairwise approaches\n",
    "    pairwise_approaches = pairwise_df['Prompting_Approach'].unique()\n",
    "    for approach in pairwise_approaches:\n",
    "        print(f\"Processing approach: {approach}\")\n",
    "        summary = process_pairwise_approach(approach)\n",
    "        \n",
    "        if summary:\n",
    "            comparison_results.append({\n",
    "                'Method': f'Pairwise - {approach}',\n",
    "                'Num_Models': summary['num_models'],\n",
    "                'Mean_Pearson': f\"{summary['mean_pearson']:.3f} ± {summary['std_pearson']:.3f}\",\n",
    "                'Mean_Spearman': f\"{summary['mean_spearman']:.3f} ± {summary['std_spearman']:.3f}\",\n",
    "                'Mean_RMSE': f\"{summary['mean_rmse']:.3f} ± {summary['std_rmse']:.3f}\"\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    return comparison_df, individual_df\n",
    "\n",
    "def display_results(comparison_df, individual_df):\n",
    "    \"\"\"Display comparison results in formatted tables.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"METHOD COMPARISON: Direct vs Pairwise Approaches\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if comparison_df is not None and not comparison_df.empty:\n",
    "        print(comparison_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No comparison data available\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)  \n",
    "    print(\"INDIVIDUAL MODEL PERFORMANCE: Direct Solicitation\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if individual_df is not None and not individual_df.empty:\n",
    "        print(individual_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No individual performance data available\")\n",
    "\n",
    "def save_results(comparison_df, individual_df, output_dir=\"output\"):\n",
    "    \"\"\"Save results to CSV files.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if comparison_df is not None and not comparison_df.empty:\n",
    "        comparison_path = os.path.join(output_dir, \"method_comparison_results.csv\")\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        print(f\"\\nMethod comparison saved to: {comparison_path}\")\n",
    "    \n",
    "    if individual_df is not None and not individual_df.empty:\n",
    "        individual_path = os.path.join(output_dir, \"individual_direct_performance.csv\")\n",
    "        individual_df.to_csv(individual_path, index=False)\n",
    "        print(f\"Individual performance saved to: {individual_path}\")\n",
    "\n",
    "def analyze_performance(comparison_df):\n",
    "    \"\"\"Analyze and summarize performance differences.\"\"\"\n",
    "    \n",
    "    if comparison_df is None or comparison_df.empty:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract numerical values\n",
    "    performance_data = []\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        pearson_mean = float(row['Mean_Pearson'].split(' ± ')[0])\n",
    "        rmse_mean = float(row['Mean_RMSE'].split(' ± ')[0])\n",
    "        \n",
    "        performance_data.append({\n",
    "            'Method': row['Method'],\n",
    "            'Pearson_Mean': pearson_mean,\n",
    "            'RMSE_Mean': rmse_mean\n",
    "        })\n",
    "    \n",
    "    perf_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    # Find best performers\n",
    "    best_correlation = perf_df.loc[perf_df['Pearson_Mean'].idxmax()]\n",
    "    best_rmse = perf_df.loc[perf_df['RMSE_Mean'].idxmin()]\n",
    "    \n",
    "    print(f\"Best correlation: {best_correlation['Method']} (r = {best_correlation['Pearson_Mean']:.3f})\")\n",
    "    print(f\"Lowest error: {best_rmse['Method']} (RMSE = {best_rmse['RMSE_Mean']:.3f})\")\n",
    "    \n",
    "    # Compare direct vs pairwise\n",
    "    direct_perf = perf_df[perf_df['Method'] == 'Direct Solicitation']\n",
    "    pairwise_perf = perf_df[perf_df['Method'] != 'Direct Solicitation']\n",
    "    \n",
    "    if not direct_perf.empty and not pairwise_perf.empty:\n",
    "        direct_corr = direct_perf['Pearson_Mean'].iloc[0]\n",
    "        best_pairwise_corr = pairwise_perf['Pearson_Mean'].max()\n",
    "        best_pairwise_method = pairwise_perf.loc[pairwise_perf['Pearson_Mean'].idxmax(), 'Method']\n",
    "        \n",
    "        print(f\"\\nDirect vs Best Pairwise:\")\n",
    "        print(f\"  Direct: {direct_corr:.3f}\")\n",
    "        print(f\"  Best Pairwise ({best_pairwise_method}): {best_pairwise_corr:.3f}\")\n",
    "        print(f\"  Difference: {direct_corr - best_pairwise_corr:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define expert weights (modify this dictionary for your criteria)\n",
    "    expert_weights = {\n",
    "        'Solar Radiation': 0.235,\n",
    "        'Aspect (Slope Direction)': 0.146,\n",
    "        'Land Use': 0.144,\n",
    "        'Slope': 0.075,\n",
    "        'Distance to Power Transmission Lines': 0.064,\n",
    "        'Distance to Settlements': 0.064,\n",
    "        'Distance to Transformers': 0.058,\n",
    "        'Altitude': 0.048,\n",
    "        'Distance to Protected Areas': 0.040,\n",
    "        'Distance to Highways': 0.035,\n",
    "        'Distance from Fault Lines': 0.029,\n",
    "        'Distance to Airports': 0.022,\n",
    "        'Distance to Rivers and Lakes': 0.022,\n",
    "        'Distance to Bird Migration Routes': 0.018\n",
    "    }\n",
    "    \n",
    "    # File paths (modify these for your data files)\n",
    "    pairwise_file = \"individual_weights_iterations.csv\"  \n",
    "    direct_file = \"direct_solicitation_weights.csv\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"LLM PERFORMANCE COMPARISON TOOL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run analysis\n",
    "    comparison_results, individual_results = calculate_performance_comparison(\n",
    "        pairwise_file, direct_file, expert_weights\n",
    "    )\n",
    "    \n",
    "    if comparison_results is not None and individual_results is not None:\n",
    "        # Display results\n",
    "        display_results(comparison_results, individual_results)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(comparison_results, individual_results)\n",
    "        \n",
    "        # Analyze performance\n",
    "        analyze_performance(comparison_results)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Processed {len(individual_results)} models for direct solicitation\")\n",
    "        print(f\"Compared {len(comparison_results)} different methods\")\n",
    "        print(\"Results saved to 'output' directory\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Analysis failed. Please check your input files and expert weights dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5b3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
