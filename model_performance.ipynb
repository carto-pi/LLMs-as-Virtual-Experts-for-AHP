{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, gmean\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "class LLMPerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    A tool for analyzing LLM performance in multi-criteria decision making tasks.\n",
    "    Compares LLM-generated weights against expert consensus using various statistical metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expert_weights_dict):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with expert consensus weights.\n",
    "        \n",
    "        Parameters:\n",
    "        expert_weights_dict (dict): Dictionary mapping criterion names to expert weights\n",
    "        \"\"\"\n",
    "        self.expert_weights = expert_weights_dict\n",
    "        self.results = {}\n",
    "    \n",
    "    def safe_geometric_mean(self, values):\n",
    "        \"\"\"\n",
    "        Calculate geometric mean safely, handling zeros and negative values.\n",
    "        \n",
    "        Parameters:\n",
    "        values (array-like): Values to calculate geometric mean for\n",
    "        \n",
    "        Returns:\n",
    "        float: Geometric mean, or arithmetic mean if geometric mean cannot be calculated\n",
    "        \"\"\"\n",
    "        values = np.array(values)\n",
    "        \n",
    "        # Check for non-positive values\n",
    "        if np.any(values <= 0):\n",
    "            warnings.warn(\"Found zero or negative values. Using arithmetic mean instead of geometric mean.\")\n",
    "            return np.mean(values)\n",
    "        \n",
    "        try:\n",
    "            return gmean(values)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error calculating geometric mean: {e}. Using arithmetic mean instead.\")\n",
    "            return np.mean(values)\n",
    "    \n",
    "    def calculate_performance_metrics(self, weights_df, cr_df, group_by='Method'):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics comparing LLM outputs to expert consensus.\n",
    "        \n",
    "        Parameters:\n",
    "        weights_df (DataFrame): DataFrame containing LLM weights with columns for criteria\n",
    "        cr_df (DataFrame): DataFrame containing consistency ratios\n",
    "        group_by (str): Column name to group by (e.g., 'Method', 'Model', 'Approach')\n",
    "        \n",
    "        Returns:\n",
    "        dict: Dictionary mapping group values to performance DataFrames\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get available grouping values\n",
    "        if group_by not in weights_df.columns:\n",
    "            print(f\"Available columns: {list(weights_df.columns)}\")\n",
    "            raise ValueError(f\"Column '{group_by}' not found in weights DataFrame\")\n",
    "        \n",
    "        groups = weights_df[group_by].unique()\n",
    "        print(f\"Found groups: {groups}\")\n",
    "        \n",
    "        # Get criteria columns (exclude metadata columns)\n",
    "        metadata_columns = [group_by, 'Model', 'Iteration', 'Run', 'ID']\n",
    "        criteria_columns = [col for col in weights_df.columns \n",
    "                          if col not in metadata_columns]\n",
    "        print(f\"Found {len(criteria_columns)} criteria columns\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(f\"Processing {len(groups)} groups for {group_by}\")\n",
    "        \n",
    "        for group_value in groups:\n",
    "            print(f\"\\nProcessing group: {group_value}\")\n",
    "            \n",
    "            # Filter data for the specific group\n",
    "            weights_filtered = weights_df[weights_df[group_by] == group_value].copy()\n",
    "            cr_filtered = cr_df[cr_df[group_by] == group_value].copy() if not cr_df.empty else pd.DataFrame()\n",
    "            \n",
    "            if weights_filtered.empty:\n",
    "                print(f\"Warning: No data found for {group_by}: {group_value}\")\n",
    "                continue\n",
    "            \n",
    "            # Get unique models/entities within this group\n",
    "            if 'Model' in weights_filtered.columns:\n",
    "                entities = weights_filtered['Model'].unique()\n",
    "                entity_col = 'Model'\n",
    "            else:\n",
    "                # If no Model column, treat each row as a separate entity\n",
    "                weights_filtered['Entity'] = range(len(weights_filtered))\n",
    "                entities = weights_filtered['Entity'].unique()\n",
    "                entity_col = 'Entity'\n",
    "            \n",
    "            group_results = []\n",
    "            \n",
    "            for entity in entities:\n",
    "                # Get entity data\n",
    "                entity_weights = weights_filtered[weights_filtered[entity_col] == entity]\n",
    "                \n",
    "                # Calculate mean CR if available\n",
    "                mean_cr = None\n",
    "                if not cr_filtered.empty and entity_col in cr_filtered.columns:\n",
    "                    entity_cr = cr_filtered[cr_filtered[entity_col] == entity]\n",
    "                    if not entity_cr.empty and 'CR' in entity_cr.columns:\n",
    "                        mean_cr = entity_cr['CR'].mean()\n",
    "                \n",
    "                # Calculate geometric mean weights across iterations\n",
    "                geometric_mean_weights = {}\n",
    "                \n",
    "                for col in criteria_columns:\n",
    "                    if col in entity_weights.columns:\n",
    "                        values = entity_weights[col].dropna().values\n",
    "                        if len(values) > 0:\n",
    "                            geometric_mean_weights[col] = self.safe_geometric_mean(values)\n",
    "                \n",
    "                if not geometric_mean_weights:\n",
    "                    print(f\"Warning: No valid weights found for entity {entity}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare expert weights in the same order as data columns\n",
    "                expert_weights_ordered = []\n",
    "                llm_weights_ordered = []\n",
    "                \n",
    "                for col in criteria_columns:\n",
    "                    if col in self.expert_weights and col in geometric_mean_weights:\n",
    "                        expert_weights_ordered.append(self.expert_weights[col])\n",
    "                        llm_weights_ordered.append(geometric_mean_weights[col])\n",
    "                \n",
    "                if len(expert_weights_ordered) < 2:\n",
    "                    print(f\"Warning: Insufficient matching criteria for entity {entity}\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate performance metrics\n",
    "                try:\n",
    "                    pearson_r, pearson_p = pearsonr(expert_weights_ordered, llm_weights_ordered)\n",
    "                    spearman_r, spearman_p = spearmanr(expert_weights_ordered, llm_weights_ordered)\n",
    "                    \n",
    "                    # Calculate RMSE\n",
    "                    rmse = np.sqrt(np.mean((np.array(expert_weights_ordered) - \n",
    "                                          np.array(llm_weights_ordered))**2))\n",
    "                    \n",
    "                    result_row = {\n",
    "                        'Entity': entity,\n",
    "                        'Pearson_r': round(pearson_r, 3),\n",
    "                        'Pearson_p': pearson_p,\n",
    "                        'Spearman_rho': round(spearman_r, 3),\n",
    "                        'Spearman_p': spearman_p,\n",
    "                        'RMSE': round(rmse, 3),\n",
    "                        'N_Criteria': len(expert_weights_ordered)\n",
    "                    }\n",
    "                    \n",
    "                    if mean_cr is not None:\n",
    "                        result_row['Mean_CR'] = round(mean_cr, 3)\n",
    "                    \n",
    "                    group_results.append(result_row)\n",
    "                    \n",
    "                    print(f\"  {entity}: Pearson r={pearson_r:.3f}, RMSE={rmse:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating metrics for entity {entity}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            results[group_value] = pd.DataFrame(group_results)\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def format_p_values(self, df, p_columns=['Pearson_p', 'Spearman_p']):\n",
    "        \"\"\"\n",
    "        Format p-values for display (show <0.001 for very small p-values).\n",
    "        \n",
    "        Parameters:\n",
    "        df (DataFrame): DataFrame containing p-value columns\n",
    "        p_columns (list): List of p-value column names\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: DataFrame with formatted p-value columns\n",
    "        \"\"\"\n",
    "        df_formatted = df.copy()\n",
    "        \n",
    "        for col in p_columns:\n",
    "            if col in df_formatted.columns:\n",
    "                df_formatted[col + '_formatted'] = df_formatted[col].apply(\n",
    "                    lambda x: \"<0.001\" if x < 0.001 else f\"{x:.3f}\"\n",
    "                )\n",
    "        \n",
    "        return df_formatted\n",
    "    \n",
    "    def generate_summary_statistics(self, results=None):\n",
    "        \"\"\"\n",
    "        Generate summary statistics across all groups.\n",
    "        \n",
    "        Parameters:\n",
    "        results (dict): Results dictionary (uses self.results if None)\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Summary statistics for each group\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.results\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for group_name, group_df in results.items():\n",
    "            if group_df.empty:\n",
    "                continue\n",
    "            \n",
    "            summary = {\n",
    "                'Group': group_name,\n",
    "                'N_Entities': len(group_df),\n",
    "                'Mean_Pearson_r': group_df['Pearson_r'].mean(),\n",
    "                'Std_Pearson_r': group_df['Pearson_r'].std(),\n",
    "                'Mean_Spearman_rho': group_df['Spearman_rho'].mean(),\n",
    "                'Std_Spearman_rho': group_df['Spearman_rho'].std(),\n",
    "                'Mean_RMSE': group_df['RMSE'].mean(),\n",
    "                'Std_RMSE': group_df['RMSE'].std(),\n",
    "                'Best_Pearson_r': group_df['Pearson_r'].max(),\n",
    "                'Best_Entity': group_df.loc[group_df['Pearson_r'].idxmax(), 'Entity']\n",
    "            }\n",
    "            \n",
    "            if 'Mean_CR' in group_df.columns:\n",
    "                summary['Mean_CR'] = group_df['Mean_CR'].mean()\n",
    "                summary['Std_CR'] = group_df['Mean_CR'].std()\n",
    "            \n",
    "            summary_data.append(summary)\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "    \n",
    "    def save_results(self, output_dir=\"output\", filename_prefix=\"llm_analysis\"):\n",
    "        \"\"\"\n",
    "        Save analysis results to CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        output_dir (str): Directory to save files in\n",
    "        filename_prefix (str): Prefix for output filenames\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save individual group results\n",
    "        for group_name, group_df in self.results.items():\n",
    "            if not group_df.empty:\n",
    "                # Clean group name for filename\n",
    "                clean_name = \"\".join(c for c in group_name if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "                clean_name = clean_name.replace(' ', '_').lower()\n",
    "                \n",
    "                filename = f\"{filename_prefix}_{clean_name}.csv\"\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                # Format for display\n",
    "                display_df = self.format_p_values(group_df)\n",
    "                display_df.to_csv(filepath, index=False)\n",
    "                print(f\"Saved {group_name} results to {filepath}\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_df = self.generate_summary_statistics()\n",
    "        summary_filepath = os.path.join(output_dir, f\"{filename_prefix}_summary.csv\")\n",
    "        summary_df.to_csv(summary_filepath, index=False)\n",
    "        print(f\"Saved summary statistics to {summary_filepath}\")\n",
    "        \n",
    "        return summary_df\n",
    "    \n",
    "    def display_results(self, max_groups=None):\n",
    "        \"\"\"\n",
    "        Display analysis results in a formatted way.\n",
    "        \n",
    "        Parameters:\n",
    "        max_groups (int): Maximum number of groups to display (None for all)\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"LLM PERFORMANCE ANALYSIS RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        groups_to_show = list(self.results.keys())[:max_groups] if max_groups else self.results.keys()\n",
    "        \n",
    "        for group_name in groups_to_show:\n",
    "            group_df = self.results[group_name]\n",
    "            if group_df.empty:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nGroup: {group_name}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Format p-values for display\n",
    "            display_df = self.format_p_values(group_df)\n",
    "            \n",
    "            # Select columns for display\n",
    "            display_columns = ['Entity', 'Pearson_r', 'Pearson_p_formatted', \n",
    "                             'Spearman_rho', 'Spearman_p_formatted', 'RMSE']\n",
    "            if 'Mean_CR' in display_df.columns:\n",
    "                display_columns.append('Mean_CR')\n",
    "            \n",
    "            # Only show columns that exist\n",
    "            display_columns = [col for col in display_columns if col in display_df.columns]\n",
    "            \n",
    "            print(display_df[display_columns].to_string(index=False))\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        summary_df = self.generate_summary_statistics()\n",
    "        if not summary_df.empty:\n",
    "            # Round numeric columns for display\n",
    "            numeric_columns = summary_df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_columns:\n",
    "                if col != 'N_Entities':\n",
    "                    summary_df[col] = summary_df[col].round(3)\n",
    "            \n",
    "            print(summary_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "\n",
    "\n",
    "def load_and_analyze(weights_file, expert_weights_dict, cr_file=None, \n",
    "                    group_by='Method', output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Convenience function to load data and run analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    weights_file (str): Path to CSV file containing LLM weights\n",
    "    expert_weights_dict (dict): Dictionary of expert consensus weights\n",
    "    cr_file (str): Path to CSV file containing consistency ratios (optional)\n",
    "    group_by (str): Column name to group analysis by\n",
    "    output_dir (str): Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    LLMPerformanceAnalyzer: Configured analyzer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        weights_df = pd.read_csv(weights_file)\n",
    "        print(f\"Loaded weights data: {weights_df.shape}\")\n",
    "        print(f\"Columns in weights file: {list(weights_df.columns)}\")\n",
    "        \n",
    "        cr_df = pd.DataFrame()\n",
    "        if cr_file and os.path.exists(cr_file):\n",
    "            cr_df = pd.read_csv(cr_file)\n",
    "            print(f\"Loaded consistency ratio data: {cr_df.shape}\")\n",
    "            print(f\"Columns in CR file: {list(cr_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = LLMPerformanceAnalyzer(expert_weights_dict)\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.calculate_performance_metrics(weights_df, cr_df, group_by)\n",
    "    \n",
    "    # Display and save results\n",
    "    analyzer.display_results()\n",
    "    summary_df = analyzer.save_results(output_dir)\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Expert weights from research (modify these for your use case)\n",
    "    expert_weights = {\n",
    "        'Solar Radiation': 0.235,\n",
    "        'Aspect (Slope Direction)': 0.146,\n",
    "        'Land Use': 0.144,\n",
    "        'Slope': 0.075,\n",
    "        'Distance to Power Transmission Lines': 0.064,\n",
    "        'Distance to Settlements': 0.064,\n",
    "        'Distance to Transformers': 0.058,\n",
    "        'Altitude': 0.048,\n",
    "        'Distance to Protected Areas': 0.040,\n",
    "        'Distance to Highways': 0.035,\n",
    "        'Distance from Fault Lines': 0.029,\n",
    "        'Distance to Airports': 0.022,\n",
    "        'Distance to Rivers and Lakes': 0.022,\n",
    "        'Distance to Bird Migration Routes': 0.018\n",
    "    }\n",
    "    \n",
    "    # File paths\n",
    "    consistency_ratios_path = \"data/consistency_ratios.csv\"\n",
    "    consolidated_weights_path = \"data/individual_weights_iterations.csv\"\n",
    "    \n",
    "    print(\"LLM Performance Analysis Tool\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(consolidated_weights_path):\n",
    "        print(f\"Error: Could not find weights file: {consolidated_weights_path}\")\n",
    "        print(\"Please make sure the file exists in the current directory.\")\n",
    "    elif not os.path.exists(consistency_ratios_path):\n",
    "        print(f\"Warning: Could not find consistency ratios file: {consistency_ratios_path}\")\n",
    "        print(\"Proceeding without consistency ratio analysis.\")\n",
    "        \n",
    "        # Run analysis without CR file\n",
    "        analyzer = load_and_analyze(\n",
    "            weights_file=consolidated_weights_path,\n",
    "            expert_weights_dict=expert_weights,\n",
    "            cr_file=None,\n",
    "            group_by='Prompting_Approach',  # Assuming this column exists\n",
    "            output_dir=\"output\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Found both data files. Running full analysis...\")\n",
    "        \n",
    "        # Run complete analysis\n",
    "        analyzer = load_and_analyze(\n",
    "            weights_file=consolidated_weights_path,\n",
    "            expert_weights_dict=expert_weights,\n",
    "            cr_file=consistency_ratios_path,\n",
    "            group_by='Prompting_Approach',  # Common column name\n",
    "            output_dir=\"output\"\n",
    "        )\n",
    "    \n",
    "    if analyzer:\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "        print(\"Results saved to 'output' directory.\")\n",
    "    else:\n",
    "        print(\"\\nAnalysis failed. Please check your data files and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
